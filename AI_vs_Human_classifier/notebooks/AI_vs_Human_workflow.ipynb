{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b34b65a7",
   "metadata": {},
   "source": [
    "# AI vs Human Text Classifier Workflow\n",
    "\n",
    "This notebook demonstrates the full workflow: data loading, preprocessing, feature extraction with transformers, model training, evaluation, and LIME explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c4b2d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "We will use pandas, numpy, scikit-learn, transformers, torch, and lime for this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import lime\n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b6ae3",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample dataset\n",
    "df = pd.read_csv('../data/sample_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b96ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df['label_enc'] = le.fit_transform(df['label'])\n",
    "df[['label', 'label_enc']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7969f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label_enc'], test_size=0.2, random_state=42)\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321f5af",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe14aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BERT to extract features (CLS token)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_features(texts, tokenizer, model, max_length=128):\n",
    "    features = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "        features.append(cls_embedding.squeeze())\n",
    "    return np.array(features)\n",
    "\n",
    "X_train_bert = get_bert_features(X_train, tokenizer, model)\n",
    "X_test_bert = get_bert_features(X_test, tokenizer, model)\n",
    "X_train_bert.shape, X_test_bert.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b9288",
   "metadata": {},
   "source": [
    "## 4. Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a68a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_bert, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772172a",
   "metadata": {},
   "source": [
    "## 5. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "y_pred = clf.predict(X_test_bert)\n",
    "y_proba = clf.predict_proba(X_test_bert)[:, 1]\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"Accuracy: {acc:.2f}, F1: {f1:.2f}, ROC-AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c77e24e",
   "metadata": {},
   "source": [
    "## 6. Explain Predictions with LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889dc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME explanation for a test instance\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train_bert,\n",
    "    feature_names=[f'feat_{i}' for i in range(X_train_bert.shape[1])],\n",
    "    class_names=le.classes_,\n",
    "    discretize_continuous=True\n",
    ")\n",
    "\n",
    "idx = 0  # Index of test instance to explain\n",
    "exp = explainer.explain_instance(X_test_bert[idx], clf.predict_proba, num_features=10)\n",
    "exp.show_in_notebook()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
